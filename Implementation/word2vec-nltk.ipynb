{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 28 04:35:33 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN Xp            Off  | 00000000:17:00.0 Off |                  N/A |\r\n",
      "|  0%   25C    P8     9W / 250W |    661MiB / 12196MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  TITAN Xp            Off  | 00000000:65:00.0 Off |                  N/A |\r\n",
      "|  0%   32C    P8     8W / 250W |     12MiB / 12196MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  TITAN Xp            Off  | 00000000:66:00.0  On |                  N/A |\r\n",
      "|  0%   26C    P5    15W / 250W |    267MiB / 12192MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  TITAN Xp            Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "|  0%   28C    P8     7W / 250W |     12MiB / 12196MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.45.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2020.4.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.14.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install nltk\n",
    "!pip install nltk\n",
    "\n",
    "# # Make Directory nltk_data for Download nltk\n",
    "# !mkdir nltk_data\n",
    "\n",
    "# # Download nltk book on Directory nltk_data\n",
    "# !python -m nltk.downloader -d ./nltk_data book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Downloaded nltk directory\n",
    "import nltk\n",
    "nltk.data.path.append('./nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "# Ignore Warning\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences : [[word, word, ...], [word, word, ...], ...]\n",
    "sentences = list()\n",
    "    \n",
    "# Use All book data for Training\n",
    "for raw_sentence in nltk.corpus.gutenberg.sents('shakespeare-hamlet.txt'):\n",
    "    sentences.append([word.lower() for word in raw_sentence if re.match('^[a-zA-Z]+', word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_set : set of words in sentences\n",
    "vocab_set = set()\n",
    "total_word = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    vocab_set.update(set(sentence))\n",
    "    total_word += len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30266\n",
      "4699\n"
     ]
    }
   ],
   "source": [
    "print(total_word)\n",
    "print(len(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2id = {vocab:i for i, vocab in enumerate(vocab_set)}\n",
    "id2vocab = {i:vocab for i, vocab in enumerate(vocab_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dim = len(vocab_set)\n",
    "\n",
    "# mode : 'cbow' or 'skip-gram'\n",
    "mode = 'skip-gram'\n",
    "\n",
    "# int embed_dim : dimension of embedding layer\n",
    "embed_dim = 70\n",
    "\n",
    "# bool sparse : activate/deactivate embedding layer sparse\n",
    "sparse = False\n",
    "\n",
    "# int C : window size\n",
    "C = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(mode, sentences, vocab2id, window_size=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "Mode : skip-gram, Embed dim : 70, Sparse : False, C : 3\n",
      "Epoch 1 Started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0af3e4045a4d629e7bf4a418612e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0, Loss : 8.976986\n",
      "Iteration : 50000, Loss : 1.676742\n",
      "Iteration : 100000, Loss : 5.453240\n",
      "\n",
      "Epoch 2 Started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29ef879862a4513a2550f078c96bda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0, Loss : 8.559054\n",
      "Iteration : 50000, Loss : 2.319291\n",
      "Iteration : 100000, Loss : 4.996808\n",
      "\n",
      "Epoch 3 Started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9f4260de3c4219b22a2aee6acb806e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0, Loss : 8.308043\n",
      "Iteration : 50000, Loss : 3.274035\n",
      "Iteration : 100000, Loss : 4.634839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# epoch, learning rate, scheduler are fixed (if you want to change, edit train.py)\n",
    "\n",
    "model = word2vec(mode, vocab_dim, embed_dim, sparse)\n",
    "print('*' * 45)\n",
    "print('Mode : {}, Embed dim : {}, Sparse : {}, C : {}'.format(mode, embed_dim, sparse, C))\n",
    "model.train(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'horse'\n",
    "target in vocab_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance\n",
      "*********************************************\n",
      "Mode : skip-gram, Embed dim : 70, Sparse : False, C : 3\n",
      "Top 1 word : you    score : 7.92763\n",
      "Top 2 word : tooke    score : 10.033244\n",
      "Top 3 word : princes    score : 10.88969\n",
      "Top 4 word : lunacie    score : 11.051412\n",
      "Top 5 word : wager    score : 11.081146\n",
      "Top 6 word : dreadfull    score : 11.150335\n",
      "Top 7 word : barr    score : 11.242523\n",
      "Top 8 word : assistant    score : 11.594965\n",
      "Top 9 word : stoopes    score : 11.672582\n",
      "Top 10 word : strick    score : 11.834057\n",
      "Top 11 word : forgiuenesse    score : 12.008977\n",
      "Top 12 word : affliction    score : 12.076011\n",
      "Top 13 word : reueale    score : 12.22012\n",
      "Top 14 word : doue    score : 12.305252\n",
      "Top 15 word : dominions    score : 13.264203\n"
     ]
    }
   ],
   "source": [
    "print('Euclidean Distance')\n",
    "print('*' * 45)\n",
    "print('Mode : {}, Embed dim : {}, Sparse : {}, C : {}'.format(mode, embed_dim, sparse, C))\n",
    "for i, (score, vocab) in enumerate(word_euclidean(model, target, vocab_set, vocab2id)):\n",
    "    print('Top {} word : {}    score : {}'.format(i+1, vocab, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "*********************************************\n",
      "Mode : skip-gram, Embed dim : 70, Sparse : False, C : 3\n",
      "Top 1 word : deeds    score : 0.323547\n",
      "Top 2 word : ha    score : 0.325871\n",
      "Top 3 word : rocke    score : 0.327787\n",
      "Top 4 word : sounding    score : 0.327995\n",
      "Top 5 word : going    score : 0.328931\n",
      "Top 6 word : grosser    score : 0.329292\n",
      "Top 7 word : you    score : 0.331229\n",
      "Top 8 word : starres    score : 0.332285\n",
      "Top 9 word : lead    score : 0.335527\n",
      "Top 10 word : document    score : 0.34243\n",
      "Top 11 word : pat    score : 0.346876\n",
      "Top 12 word : color    score : 0.348081\n",
      "Top 13 word : besides    score : 0.350819\n",
      "Top 14 word : pretty    score : 0.359215\n",
      "Top 15 word : mingled    score : 0.370598\n"
     ]
    }
   ],
   "source": [
    "print('Cosine Similarity')\n",
    "print('*' * 45)\n",
    "print('Mode : {}, Embed dim : {}, Sparse : {}, C : {}'.format(mode, embed_dim, sparse, C))\n",
    "for i, (score, vocab) in enumerate(word_cosine(model, target, vocab_set, vocab2id)):\n",
    "    print('Top {} word : {}    score : {}'.format(i+1, vocab, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
